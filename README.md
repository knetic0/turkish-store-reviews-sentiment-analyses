# ğŸ“ TÃ¼rkÃ§e ÃœrÃ¼n YorumlarÄ± Duygu Analizi

Bu proje, TÃ¼rkÃ§e Ã¼rÃ¼n yorumlarÄ±nÄ±n duygu analizini (Pozitif mi? NÃ¶tr mÃ¼? yoksa Negatif mi?) yapan ve arka plan'da birÃ§ok derin Ã¶ÄŸrenme
algoritmasÄ± barÄ±ndÄ±ran bir projedir. LSTM, Bidirectional LSTM, GRU algoritmalarÄ±yla eÄŸitilen birden fazla model kullanÄ±labilir. UI KÄ±s
mÄ±nda ise Gradio kullanÄ±lmÄ±ÅŸtÄ±r. KullanÄ±cÄ± istediÄŸi model'i seÃ§ebilir ve ardÄ±ndan yorum girip yorumun analizini yapabilir.

## ğŸš€ ArayÃ¼z

![KullanÄ±cÄ± ArayÃ¼zÃ¼](/images/ui-image.png)

## ğŸ”¥ KullanÄ±lan Algoritmalar

1. **LSTM**

2. **Bidirectional LSTM**

3. **GRU**

## ğŸ“ˆ KullanÄ±lan Veri Seti

Modelin eÄŸitimi sÄ±rasÄ±nda yaklaÅŸÄ±k olarak 450.000 veri, testinde ise 50.000 veri kullanÄ±lmÄ±ÅŸtÄ±r.

![Dataset](/images/dataset-example.png)

Bu gÃ¶rsel veri setinin ilk 5 satÄ±rÄ±nÄ± gÃ¶steriyor. Veri seti 3 adet sÃ¼tundan oluÅŸmaktadÄ±r. Bu sÃ¼tunlar text, label, dataset olarak nitelendirilmiÅŸtir. Burada bizi ilgilendiren kÄ±sÄ±m text ve label'dÄ±r.

[Bu link ile veri setine eriÅŸebilirsiniz!](https://www.kaggle.com/datasets/winvoker/turkishsentimentanalysisdataset)

## âœ¨ EÄŸitim OrtamÄ±

**DonanÄ±m Bilgisi**

Model eÄŸitimi iÃ§in Google Colab kullanÄ±lmÄ±ÅŸtÄ±r. Burada yer alan;

1. A100 40 GB VRAM GPU KullanÄ±lmÄ±ÅŸtÄ±r. 
2. L4 GPU kullanÄ±lmÄ±ÅŸtÄ±r.
3. T4 GPU KullanÄ±lmÄ±ÅŸtÄ±r.

## ğŸš€ EÄŸitim AyarlarÄ±

3 Adet farklÄ± algoritma kullanÄ±lmÄ±ÅŸtÄ±r. Ã–ncelikle LSTM AlgoritmasÄ±na deÄŸinebiliriz.

**1. LSTM AlgoritmasÄ± AyarlarÄ±**

```python
class LSTMClassifier(nn.Module):  # LSTM tabanlÄ± sÄ±nÄ±flandÄ±rÄ±cÄ± sÄ±nÄ±fÄ±
    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, pad_idx, n_layers=1, dropout=0.3):  # SÄ±nÄ±f kurucu metodu
        super().__init__()  # Ãœst sÄ±nÄ±fÄ±n kurucu metodunu Ã§aÄŸÄ±r
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)  # SÃ¶zcÃ¼kleri sayÄ±sal vektÃ¶rlere dÃ¶nÃ¼ÅŸtÃ¼r
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers,
                            dropout=dropout, batch_first=True)  # LSTM katmanÄ±nÄ± tanÄ±mla
        self.fc = nn.Linear(hidden_dim, output_dim)  # Son katmanda sÄ±nÄ±flandÄ±rma yap
        self.dropout = nn.Dropout(dropout)  # AÅŸÄ±rÄ± Ã¶ÄŸrenmeyi engellemek iÃ§in dropout uygula

    def forward(self, text, lengths):  # Ä°leri yayÄ±lÄ±m fonksiyonu
        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)  # DeÄŸiÅŸken uzunluktaki dizileri paketle
        packed_output, (hidden, _) = self.lstm(packed_embedded)  # LSTM katmanÄ±ndan geÃ§
        output = self.fc(self.dropout(hidden[-1]))  # Dropout sonrasÄ± tam baÄŸlantÄ±lÄ± katman
        return output  # Son tahmin deÄŸerini dÃ¶ndÃ¼r
```

**2. Bidirectional LSTM AlgoritmasÄ± AyarlarÄ±**

```python
class BiLSTM(nn.Module):  # BiLSTM sÄ±nÄ±fÄ±, PyTorch'un nn.Module sÄ±nÄ±fÄ±ndan tÃ¼retilir
    def __init__(self, vocab_size, emb_dim, hid_dim, out_dim, pad_idx, n_layers=1, dropout=0.5):  # YapÄ±cÄ± metot, modelin genel ayarlarÄ±nÄ± yapar
        super().__init__()  # Ãœst sÄ±nÄ±fÄ±n yapÄ±cÄ±sÄ±nÄ± Ã§aÄŸÄ±rÄ±r
        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)  # GiriÅŸ metinlerini sayÄ±sal vektÃ¶rlere dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r
        self.lstm = nn.LSTM(
            emb_dim,        # GiriÅŸ katman boyutu
            hid_dim,        # Gizli katman boyutu
            num_layers=n_layers,    # LSTM katman sayÄ±sÄ±
            bidirectional=True,     # Ã‡ift yÃ¶nlÃ¼ LSTM
            batch_first=True,       # Veri sÄ±rasÄ±: (batch, seq, feature)
            dropout=dropout if n_layers>1 else 0.0  # Katman sayÄ±sÄ±na gÃ¶re dropout oranÄ±
        )
        self.fc = nn.Linear(hid_dim * 2, out_dim)  # Ã‡ift yÃ¶nlÃ¼ LSTM Ã§Ä±ktÄ±sÄ±nÄ± sÄ±nÄ±flandÄ±rma katmanÄ±na gÃ¶nderir
        self.drop = nn.Dropout(dropout)  # AÅŸÄ±rÄ± Ã¶ÄŸrenmeyi Ã¶nlemek iÃ§in dropout uygular

    def forward(self, texts, lengths):  # Ä°leri yayÄ±lÄ±m fonksiyonu
        emb = self.drop(self.embedding(texts))  # Metin gÃ¶mmeleri oluÅŸturur ve dropout uygular
        packed = pack_padded_sequence(emb, lengths.cpu(), batch_first=True, enforce_sorted=False)  # DeÄŸiÅŸken uzunluktaki dizileri paketler
        _, (hidden, _) = self.lstm(packed)  # LSTM katmanÄ±nÄ± Ã§alÄ±ÅŸtÄ±rarak gizli durum bilgisine ulaÅŸÄ±r
        h_cat = torch.cat((hidden[-2], hidden[-1]), dim=1)  # Ä°leri ve geri yÃ¶nlÃ¼ Ã§Ä±ktÄ±larÄ± birleÅŸtirir
        return self.fc(self.drop(h_cat))  # Ã‡Ä±ktÄ±yÄ± tam baÄŸlÄ± katmandan geÃ§irir ve dÃ¶ndÃ¼rÃ¼r
```

**3. GRU AlgoritmasÄ± AyarlarÄ±**

```python
class GRUClassifier(nn.Module):  # GRU tabanlÄ± sÄ±nÄ±flandÄ±rÄ±cÄ± sÄ±nÄ±fÄ±
    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, pad_idx):  # SÄ±nÄ±fÄ± baÅŸlatmak iÃ§in gerekli parametreleri tanÄ±mla
        super().__init__()  # Ãœst sÄ±nÄ±fÄ±n kurucu metodunu Ã§aÄŸÄ±r
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)  # Metinleri sayÄ±sal vektÃ¶rlere dÃ¶nÃ¼ÅŸtÃ¼r
        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True, bidirectional=True)  # Ä°ki yÃ¶nlÃ¼ (bidirectional) GRU katmanÄ± tanÄ±mla
        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # Ã‡ift yÃ¶nlÃ¼ GRU Ã§Ä±ktÄ±sÄ±nÄ± sÄ±nÄ±flandÄ±rma katmanÄ±na aktar
        self.dropout = nn.Dropout(0.3)  # Overfitting'i Ã¶nlemek iÃ§in dropout uygula

    def forward(self, input_ids):  # Modelin ileri yayÄ±lÄ±m fonksiyonu
        x = self.embedding(input_ids)  # GiriÅŸleri embedding katmanÄ±ndan geÃ§ir
        _, h = self.gru(x)  # GRU katmanÄ±ndan Ã§Ä±ktÄ± al, yalnÄ±zca gizli durum kullan
        h = torch.cat((h[-2], h[-1]), dim=1)  # Ä°leri ve geri yÃ¶nlÃ¼ son katmanlarÄ± birleÅŸtir
        h = self.dropout(h)  # Dropout uygula
        return self.fc(h)  # Ã‡Ä±ktÄ±yÄ± sÄ±nÄ±flandÄ±rma katmanÄ±ndan geÃ§ir ve dÃ¶ndÃ¼r
```

## ğŸŒ EÄŸitim Parametreleri

**1. Bidirectional LSTM Parametreleri**

| Parameter | DeÄŸer |
|-----------|-------|
| Learning rate | 1e-3 |
| Batch size | 128 |
| Maximum epochs | 5 |
| Loss function | SÄ±nÄ±f aÄŸÄ±rlÄ±klÄ± Cross-Entropy Loss |
| Optimizer | Adam |
| Maximum sequence length | 128 token |
| Device | CUDA (mevcut ise) / CPU |

**2. LSTM Parametreleri**

| Parameter | DeÄŸer |
|-----------|-------|
| Learning rate | 1e-3 |
| Batch size | 128 |
| Maximum epochs | 10 |
| Loss function | SÄ±nÄ±f aÄŸÄ±rlÄ±klÄ± Cross-Entropy Loss |
| Optimizer | Adam |
| Maximum sequence length | 128 token |
| Device | CUDA (mevcut ise) / CPU |

**3. GRU Parametreleri**

| Parameter | DeÄŸer |
|-----------|-------|
| Learning rate | 1e-3 |
| Batch size | 64 |
| Maximum epochs | 15 |
| Early stopping patience | 2 |
| Loss function | SÄ±nÄ±f aÄŸÄ±rlÄ±klÄ± Cross-Entropy Loss |
| Optimizer | Adam |
| Maximum sequence length | 128 token |
| Device | CUDA (mevcut ise) / CPU |

## ğŸ“ˆ EÄŸitim RaporlarÄ±

**1. Bidirectional LSTM RaporlarÄ±**

![Bidirectional LSTM Confusion Matrix](/images/bidirectional-lstm-confusion-matrix.png)

| Class           | Precision | Recall | F1-Score | Support |
|-----------------|-----------|--------|----------|--------:|
| 0               | 0.94      | 0.96   | 0.95     |   26209 |
| 1               | 0.97      | 0.99   | 0.98     |   17087 |
| 2               | 0.86      | 0.74   | 0.79     |    5655 |
| **Accuracy**    |           |        | **0.94** |   48951 |
| **Macro Avg**   | 0.92      | 0.89   | 0.91     |   48951 |
| **Weighted Avg**| 0.94      | 0.94   | 0.94     |   48951 |

**2. LSTM RaporlarÄ±**

![LSTM Confusion Matrix](/images/lstm-confusion-matrix.png)

| Class           | Precision | Recall | F1-Score | Support |
|-----------------|-----------|--------|----------|--------:|
| 0               | 0.94      | 0.96   | 0.95     |   26209 |
| 1               | 0.97      | 0.98   | 0.98     |   17087 |
| 2               | 0.86      | 0.74   | 0.80     |    5655 |
| **Accuracy**    |           |        | **0.94** |   48951 |
| **Macro Avg**   | 0.92      | 0.89   | 0.91     |   48951 |
| **Weighted Avg**| 0.94      | 0.94   | 0.94     |   48951 |


**3. GRU RaporlarÄ±**

![GRU Confusion Matrix](/images/gru-confusion-matrix.png)

| Class            | Precision | Recall | F1-Score | Support |
|------------------|-----------|--------|----------|--------:|
| Negative         | 0.75      | 0.86   | 0.80     |    5656 |
| Notr             | 0.99      | 0.99   | 0.99     |   17092 |
| Positive         | 0.97      | 0.94   | 0.95     |   26217 |
| **Accuracy**     |           |        | **0.95** |   48965 |
| **Macro Avg**    | 0.90      | 0.93   | 0.92     |   48965 |
| **Weighted Avg** | 0.95      | 0.95   | 0.95     |   48965 |

## âš¡ï¸ Ã‡alÄ±ÅŸma Bilgileri

**1. Bidirectional LSTM Ã‡alÄ±ÅŸma Bilgileri**

| Aspect | Detaylar |
|--------|----------|
| Hardware | Google Colab A100 GPU |
| Training time | 541.147 Saniye YaklaÅŸÄ±k olarak 9,02 Dakika |
| Training monitoring | Her epoch sonrasÄ±nda DoÄŸruluk, F1, Kesinlik ve Geri Ã‡aÄŸÄ±rma hesaplanÄ±r |
| Model saving | En iyi model doÄŸrulama F1 skoruna gÃ¶re kaydedilir |

**2. LSTM Ã‡alÄ±ÅŸma Bilgileri**

| Aspect | Detaylar |
|--------|----------|
| Hardware | Google Colab T4 GPU |
| Training time | 793.126 Saniye YaklaÅŸÄ±k Olarak 13.2188 Dakika |
| Training monitoring | Her epoch sonrasÄ±nda DoÄŸruluk, F1, Kesinlik ve Geri Ã‡aÄŸÄ±rma hesaplanÄ±r |
| Model saving | En iyi model doÄŸrulama F1 skoruna gÃ¶re kaydedilir |

**3. GRU Ã‡alÄ±ÅŸma Bilgileri**

| Aspect | Detaylar |
|--------|----------|
| Hardware | Google Colab A100 GPU |
| Training time | 16 dakika 21 saniye |
| Early stopping | F1 skoru 2 ardÄ±ÅŸÄ±k epoch boyunca iyileÅŸmezse etkinleÅŸir |
| Training monitoring | Her epoch sonrasÄ±nda DoÄŸruluk, F1, Kesinlik ve Geri Ã‡aÄŸÄ±rma hesaplanÄ±r |
| Model saving | En iyi model doÄŸrulama F1 skoruna gÃ¶re kaydedilir |

## ğŸŒ Dosya YapÄ±sÄ±

```bash
.
â”œâ”€â”€ app.py                                                  # Gradio tabanlÄ± duygu analizi uygulama arayÃ¼zÃ¼
â”œâ”€â”€ final_nlp_deep_learning_turkish_sentiment_analysis      # Bidirectional LSTM ile eÄŸitim          
â”œâ”€â”€ models/                                                 # Modellerin barÄ±ndÄ±rÄ±ldÄ±ÄŸÄ± klasÃ¶r
â”‚â”€â”€â”€â”€ bidirectional_lstm/
â”‚        â”œâ”€â”€ algorithm.py                                   # RNN sÄ±nÄ±fÄ±nÄ± iÃ§eren dosya
â”‚        â”œâ”€â”€ itos.pkl                                       # index-to-string sÃ¶zlÃ¼ÄŸÃ¼
â”‚        â”œâ”€â”€ model.pth                                      # EÄŸitilmiÅŸ model dosyasÄ±
â”‚        â”œâ”€â”€ stoi.pkl                                       # string-to-index sÃ¶zlÃ¼ÄŸÃ¼
â”‚â”€â”€â”€â”€ lstm/                                                
â”‚        â”œâ”€â”€ algorithm.py
â”‚        â”œâ”€â”€ itos.pkl
â”‚        â”œâ”€â”€ model.pth
â”‚        â”œâ”€â”€ stoi.pkl
â”œâ”€â”€ utils/                                                  # Ortak kullanÄ±lan metotlarÄ±n barÄ±ndÄ±rÄ±ldÄ±ÄŸÄ± klasÃ¶r
â”‚   â”œâ”€â”€ string.py
â”‚   â””â”€â”€ ...
â”œâ”€â”€ requirements.txt                                        # Gerekli Python kÃ¼tÃ¼phaneleri
â””â”€â”€ README.md                                               # Proje aÃ§Ä±klamalarÄ±
```

## ğŸ“¦ Gereksinimler

* Python >= 3.8
* AÅŸaÄŸÄ±daki kurulumlarÄ± yapmanÄ±z gerekmektedir.
* **(Opsiyonel)** CUDA Destekli Cihaz veya Google Colab PRO ÃœyeliÄŸi

## ğŸ› ï¸ Kurulum

**1. Gerekli KÃ¼tÃ¼phanelerin Kurulumu**

```bash
pip install -r requirements.txt
```

*MacOS iÃ§in;*

```bash
pip3 install -r requirements.txt
```

**2. Gradio UygulamasÄ±nÄ± Ã‡alÄ±ÅŸtÄ±rÄ±n**

```bash
python3 app.py
```

## ğŸ“š KullanÄ±lan Teknolojiler

* [PyTorch](https://pytorch.org/)
* [Scikit-Learn](https://scikit-learn.org/)
* [Gradio](https://www.gradio.app/)
* [Kagglehub](https://www.kaggle.com/)

## ğŸ“œ Lisans

Bu proje `MIT LisansÄ±` ile lisanslanmÄ±ÅŸtÄ±r. Daha fazla bilgi iÃ§in `LICENSE` dosyasÄ±na gÃ¶z atabilirsiniz.